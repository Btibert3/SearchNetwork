{
 "metadata": {
  "name": "Build Highered College Search Network"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Use the Similar Colleges Graph to Study Common Admission Metrics"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the libraries\n",
      "import urllib\n",
      "import lxml.html\n",
      "import re\n",
      "from pymongo import MongoClient \n",
      "import time\n",
      "#from bs4 import BeautifulSoup\n",
      "#from urllib2 import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to a remote MongoDB instance\n",
      "# in this case, its on another computer on my local network\n",
      "mongo = MongoClient(host='192.168.1.69')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to the database, gets created if it doesnt exist\n",
      "db = mongo['he_search_graph']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Get the Roster of Schools and the URI for each"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create the base url\n",
      "url = \"http://www.cappex.com/colleges/\"\n",
      "# get the page\n",
      "page = urllib.urlopen(url).read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a roster collection and save the raw data to mongodb\n",
      "# drop if already exists\n",
      "db.roster.drop()\n",
      "raw = {'page': page}\n",
      "db.roster.insert(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Parse the roster into a list of links"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse the page -- redundant if same session, but calls the saved data\n",
      "# assumes only one entry in the roster collection!!\n",
      "html = db.roster.find_one()\n",
      "html = html.get('page')\n",
      "dom = lxml.html.fromstring(html)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lxml to scrape\n",
      "# http://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautiful-soup\n",
      "pattern = re.compile('/colleges/[A-Z].*/$')\n",
      "colleges = []\n",
      "for link in dom.xpath('//a/@href'):\n",
      "    match = pattern.match(link)\n",
      "    if match is not None:\n",
      "        colleges.append(match.group())\n",
      "        #print 'appended another school:   ' + match.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check for the number of colleges we scraped\n",
      "len(colleges)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. For each page, grab the HTML and throw into MongoDB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BASE URL\n",
      "BASE_URL = 'http://www.cappex.com'\n",
      "\n",
      "# drop the collection if already exists\n",
      "# only really useful for debugging\n",
      "db.rawpages.drop()\n",
      "\n",
      "for college in colleges:\n",
      "    url = BASE_URL + college\n",
      "    try:\n",
      "        page = urllib.urlopen(url).read()\n",
      "    except:\n",
      "        page = \" \"\n",
      "    # create a dict\n",
      "    raw = {'page' : page,\n",
      "           'college'  : college,\n",
      "           'url': url}\n",
      "    # write the dict to raw pages collection\n",
      "    db.rawpages.insert(raw)\n",
      "    # print out status\n",
      "    print ('completed ' + college)\n",
      "    # sleep for 1 seconds\n",
      "    time.sleep(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "4. Parse the pages for the data we want"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# drop the collection - only good for debugging\n",
      "db.parsedpages.drop()\n",
      "# parse the data and throw back into MongoDB\n",
      "for school in db.rawpages.find():\n",
      "    # parse the school name\n",
      "    sname = school.get('college')\n",
      "    sname = re.sub('-', ' ', sname[10:-1])\n",
      "    # grab the other data - have to account for some missing info\n",
      "    try:\n",
      "        html = schools.get('page')\n",
      "        dom = lxml.html.fromstring(html)\n",
      "        # grab the unitid\n",
      "        tmp = dom.xpath('//*[@id=\"collegeYes\"]/button/@data-url')\n",
      "        tmp = str(tmp[0])\n",
      "        unitid = re.search('[0-9]{1,6}', tmp).group(0)\n",
      "        # get the schools rating\n",
      "        tmp  = dom.xpath('/html/body/div[1]/div[3]/div/div/div[1]/div/div[1]/div/a/span/meta[1]/@content')\n",
      "        rating = float(tmp[0])\n",
      "        # similar colleges - only grabs the first 10 - ignores \"Hidden Peers\"\n",
      "        peers = []\n",
      "        for i in range(1,11):\n",
      "            xpath2 = '//*[@id=\"siteContentWrapper\"]/div/div/div[3]/div[3]/div[3]/div/div/ul[1]/li[' + str(i) + ']/a/@href'\n",
      "            try:\n",
      "                peer = str(dom.xpath(xpath2)[0])\n",
      "                peers.append(peer)\n",
      "            except:\n",
      "                peers.append(\" \")\n",
      "        # create the ranks\n",
      "        ranks = range(1, len(peers)+1)\n",
      "    except:\n",
      "        unitid = \" \"\n",
      "        rating = \" \"\n",
      "        peers = []\n",
      "        ranks = []\n",
      "    # put together the dict\n",
      "    parsed = {'sname' : sname,\n",
      "              'unitid' : unitid,\n",
      "              'rating' : rating,\n",
      "              'peers' : peers,\n",
      "              'ranks' : ranks}\n",
      "    # throw the parsed data into a mongodb collection\n",
      "    db.parsedpages.insert(parsed)\n",
      "    # status\n",
      "    print \"completed \" + school.get('college')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Use `R` to do the last bits of cleaning and data collection in order to start putting together some datasets"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cleanup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# close the mongodb connection \n",
      "mongo.disconnect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Help and Appendix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Greg Rada's Awesome Tutorial](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}