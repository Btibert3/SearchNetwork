{
 "metadata": {
  "name": "get-data"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Use the Similar Colleges Graph to Study Common Admission Metrics"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Setup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the libraries\n",
      "import urllib\n",
      "import lxml.html\n",
      "import re\n",
      "from pymongo import MongoClient \n",
      "import time\n",
      "#from bs4 import BeautifulSoup\n",
      "#from urllib2 import urlopen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to a remote MongoDB instance\n",
      "# in this case, its on another computer on my local network\n",
      "mongo = MongoClient(host='192.168.1.69')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# connect to the database, gets created if it doesnt exist\n",
      "db = mongo['he_search_graph']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1. Get the Roster of Schools and the URI for each"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create the base url\n",
      "url = \"http://www.cappex.com/colleges/\"\n",
      "# get the page\n",
      "page = urllib.urlopen(url).read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# create a roster collection and save the raw data to mongodb\n",
      "# drop if already exists\n",
      "db.roster.drop()\n",
      "raw = {'page': page}\n",
      "db.roster.insert(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Parse the roster into a list of links"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# parse the page -- redundant if same session, but calls the saved data\n",
      "# assumes only one entry in the roster collection!!\n",
      "html = db.roster.find_one()\n",
      "html = html.get('page')\n",
      "dom = lxml.html.fromstring(html)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lxml to scrape\n",
      "# http://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautiful-soup\n",
      "pattern = re.compile('/colleges/[A-Z].*/$')\n",
      "colleges = []\n",
      "for link in dom.xpath('//a/@href'):\n",
      "    match = pattern.match(link)\n",
      "    if match is not None:\n",
      "        colleges.append(match.group())\n",
      "        #print 'appended another school:   ' + match.group()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# check for the number of colleges we scraped\n",
      "len(colleges)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "3. For each page, grab the HTML and throw into MongoDB"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# BASE URL\n",
      "BASE_URL = 'http://www.cappex.com'\n",
      "\n",
      "# drop the collection if already exists\n",
      "# only really useful for debugging\n",
      "db.rawpages.drop()\n",
      "\n",
      "for college in colleges:\n",
      "    url = BASE_URL + college\n",
      "    try:\n",
      "        page = urllib.urlopen(url).read()\n",
      "    except:\n",
      "        page = \" \"\n",
      "    # create a dict\n",
      "    raw = {'page' : page,\n",
      "           'college'  : college,\n",
      "           'url': url}\n",
      "    # write the dict to raw pages collection\n",
      "    db.rawpages.insert(raw)\n",
      "    # print out status\n",
      "    print ('completed ' + college)\n",
      "    # sleep for 1 seconds\n",
      "    time.sleep(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Next Steps"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Use `R` to parse the pages, get the IPEDS data, and analyze the data\n",
      "- NOTE: A previous version of this notebook attempted to parse the pages - for me, its easier to debug in R"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Cleanup"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# close the mongodb connection \n",
      "mongo.disconnect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Help and Appendix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Greg Rada's Awesome Tutorial](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}